from flask import Flask, jsonify
import os, datetime, requests, json, pandas as pd
from google.cloud import storage
from io import BytesIO
from datetime import timedelta
from collections import defaultdict

app = Flask(__name__)

# === CONFIG ===
client = storage.Client()
RAW_BUCKET = "gal_raw"
STAGING_BUCKET = "gal_staging"

now = datetime.datetime.now(datetime.timezone.utc).date()
year, month, day = now.strftime("%Y"), now.strftime("%m"), now.strftime("%d")
formatted_date = now.isoformat()
end_date = now.strftime("%Y-%m-%d")
today_str = now.strftime("%Y_%m_%d")

HEADERS = {"Content-Type": "application/json"}

# === CLEANUP ===
def clear_daily_update(bucket_name, prefix):
    for blob in client.bucket(bucket_name).list_blobs(prefix=prefix):
        blob.delete()
        print(f"üßπ –£–¥–∞–ª–µ–Ω–æ: {blob.name}")

# === STORAGE ===
def upload_json_and_parquet(result_json, source_path, file_prefix):
    json_name = f"{today_str}_{file_prefix}.json"
    parquet_name = json_name.replace(".json", ".parquet")
    paths = {
        "raw": f"{source_path}/archive/{year}/{month}/{json_name}",
        "stg_daily": f"{source_path}/daily_update/{parquet_name}",
        "stg_archive": f"{source_path}/archive/{year}/{month}/{parquet_name}"
    }

    clear_daily_update(STAGING_BUCKET, f"{source_path}/daily_update/")

    # JSON ‚Üí GCS
    json_data = json.dumps(result_json, ensure_ascii=False)
    client.bucket(RAW_BUCKET).blob(paths["raw"]).upload_from_string(json_data, content_type="application/json")

    # JSON ‚Üí DataFrame
    result = result_json.get("result", {})
    values = result.get("values", result) if isinstance(result, dict) else result
    if not values:
        print(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {file_prefix}")
        return

    if isinstance(values, list):
        df = pd.DataFrame(values)
    else:
        df = pd.json_normalize(values)

    if df.empty:
        print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π DataFrame –¥–ª—è {file_prefix}")
        return

    # ====== DTYPE ENFORCEMENT ======
    # –∫–∞—Ä—Ç—ã –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Å—Ö–µ–º—ã Parquet
    FLOAT_COLS_MAP = {
        "unf_sales": ["price", "quantity", "sum"],
        "unf_orders": ["payment_percent", "price", "quantity", "sum"],
        "unf_receipt_payments": ["sum"],
    }
    ID_COLS = [
        "document_id","order_id","seller_id","buyer_id",
        "nomenclature_id","variation_id","unit_id","category_id",
        "receiver_id","sender_id","contract_id",
    ]
    DATE_COLS = ["date", "document_datetime"]

    def coerce_float(df, col):
        if col in df.columns:
            # –õ—é–±—ã–µ int/str ‚Üí float64; –Ω–µ—á–∏—Å–ª–æ–≤–æ–µ ‚Üí NaN
            df[col] = pd.to_numeric(df[col], errors="coerce").astype("float64")

    # –¥–µ–Ω—å–≥–∏/–ø—Ä–æ—Ü–µ–Ω—Ç—ã/–∫–æ–ª-–≤–∞ ‚Üí float64
    for col in FLOAT_COLS_MAP.get(file_prefix, []):
        coerce_float(df, col)

    # ID ‚Üí —Å—Ç—Ä–æ–∫–∞ (—Å—Ç–∞–±–∏–ª—å–Ω–æ string, —á—Ç–æ–±—ã –Ω–µ –ø—Ä—ã–≥–∞–ª–æ –≤ INT64)
    for col in ID_COLS:
        if col in df.columns:
            df[col] = df[col].astype("string")

    # –î–∞—Ç—ã ‚Üí ISO-—Å—Ç—Ä–æ–∫–∏ (—É–¥–æ–±–Ω–æ –¥–ª—è PARSE_* –≤ BQ)
    for col in DATE_COLS:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce")
            # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑—É–µ–º/–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            try:
                # –µ—Å–ª–∏ –±–µ–∑ tz ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º; –µ—Å–ª–∏ –µ—Å—Ç—å tz ‚Äî –ø—Ä–∏–≤–µ–¥—ë–º –∫ UTC
                if getattr(df[col].dt, "tz", None) is not None:
                    df[col] = df[col].dt.tz_convert("UTC")
            except Exception:
                pass
            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É ISO (BigQuery –∑–∞—Ç–µ–º –ø–∞—Ä—Å–∏—Ç –≤ DATE/TIMESTAMP)
            df[col] = df[col].astype("string")

    # ====== DataFrame ‚Üí Parquet ======
    buf = BytesIO()
    # engine –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é fastparquet/pyarrow ‚Äî –ª—é–±–æ–π –Ω–æ—Ä–º, –Ω–æ pyarrow —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
    df.to_parquet(buf, index=False)  # –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å engine="pyarrow"
    data = buf.getvalue()

    stg = client.bucket(STAGING_BUCKET)
    stg.blob(paths["stg_daily"]).upload_from_string(data, content_type="application/octet-stream")
    stg.blob(paths["stg_archive"]).upload_from_string(data, content_type="application/octet-stream")

SOURCES = [
    {
        "endpoint": "outgoing_payments",
        "params": {
            "auth_code": os.environ.get("AUTH_CODE", ""),        
            "begin_date": "2024-01-01",
            "end_date": end_date,
            "fields": "date, sender_id, sender_name, receiver_id, receiver_name, contract_id, contract_name, sum, document_id, document_number,"
            "document_datetime, document_type, intercompany, active, operation_type, currency_id, currency_name"},
        "source_path": "unf-test/outgoing_payments",
        "file_prefix": "unf_outgoing_payments"
    },
    
       {
        "endpoint": "receipt_payments",
        "params": {
            "auth_code": os.environ.get("AUTH_CODE", ""),
            "begin_date": "2024-01-01",
            "end_date": end_date,
            "fields": "document_id, document_number, document_datetime, document_type, date, receiver_name, sender_name, contract_name, sum,receiver_id, sender_id, contract_id, intercompany, active, operation_type, currency_id, currency_name",
        "conditions": [            {
             "field": "active",
             "cond": "eq",
             "value": True
            },
            {
             "field": "operation _type",
             "cond": "eq",
             "value": "–ü–æ—Å—Ç–∞–≤—â–∏–∫—É"
            },
            {
             "field": "intercompany",
             "cond": "eq",
             "value": False
            },
            {"field": "document_id", "cond": "in", "value": document_ids}]
        },
        "source_path": "unf-test/receipt_payments",
        "file_prefix": "unf_receipt_payments"
    }
    
]

# üöÄ –ï–¥–∏–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–æ GET /
@app.route("/", methods=["GET"])
def trigger_all():
    results = []
    for src in SOURCES:
        try:
            url = f"http://192.168.53.27/api_test/{src['endpoint']}"
            resp = requests.get(url, headers={"Content-Type": "application/json"}, json=src["params"], timeout=3600)
            if resp.status_code == 200:
                upload_json_and_parquet(resp.json(), src["source_path"], src["file_prefix"])
                results.append({src["endpoint"]: "‚úÖ"})
            else:
                results.append({src["endpoint"]: f"‚ùå {resp.status_code}"})
        except Exception as e:
            results.append({src["endpoint"]: f"‚ùå {str(e)}"})

   

    return jsonify(results), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8080)))
